# FS I/O担当の方々へ

# アプリの実行方法
`make/Makefile`にコンパイル方法が示してあります。
富岳での実行を想定していますが、難しいことはやっていないので、`gfortran`や`ifx`でも動くはずです。

`make/Makefile`冒頭の
```makefile
SERVER=FUGAKU # for FUGAKU
```
でコンパイルをコントロールしています。

```makefile
PPC := $(PPC) -Dposixio_write
PPC := $(PPC) -Dposixio_read
```
が指定されているときはMPIスレッドごとにファイルに読み書きします。ここをコメントアウトするとMPI I/Oを用いて、一つのファイルに書き込むようになります。

`FS/`以下で`make`とすればコンパイルされます。`FS/sh`以下に富岳用のジョブ投入のシェルスクリプト(`fg.sh`)も提供しています。

# プログラム群
`FS/src/all`以下に全てのプログラムが配置してあります。まずは`main.F90`を見ていただくと全体的な流れがわかると思います。
主な出力部分は`io.F90`です。
チェックポイントデータは、`FS/data/qq`に、解析のためのデータはMPIスレッド内で少し配置換えをしてから`FS/data/remap/qq`に書き出しています。ディレクトリは、ジョブ実行中に生成されます。配置換えは`remap.F90`で実施しています。

格子点数は`geometry.F90`で設定しています。

```fortran
  integer, parameter, private :: nx0 = 256, ny0 = 128, nz0 = 128
```
で`nx0`, `ny0`, `nz0`はそれぞれMPIスレッドごとの格子点数(袖なし)です。

```fortran
  integer, parameter :: ix0 = 8, jx0 = 16, kx0 = 16
```
で各方向の領域分割数を決めています。`ix0*jx0*kx0`が全MPIスレッド数になるようにしています。
富岳では、1ノードあたり4MPIスレッドになるようにしていて、ジョブ投入の際
```shell
#PJM -L "node=ix0xjx0/2xkx0/2"
```
となるようにすると、`fjmpi`で富岳の実ノード配置を見てから都合の良いように並べてくれます。
これを破ると`MPI_cart_create`で決定するノード配置となるようになっています。

# 典型的計算サイズ
現在実施している最も大きな計算だと8192ノードで1ノードあたり4MPIスレッドを割り当てて、32768MPIスレッド。1MPIスレッド128x192x192程度です。

# 変更履歴
2024年1月30日 堀田英之